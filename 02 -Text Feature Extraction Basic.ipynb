{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723bdf6d",
   "metadata": {},
   "source": [
    "# Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231b631",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>What is Feature Extraction in NLP?</b>\n",
    "\n",
    "\n",
    "In NLP, feature extraction is the process of transforming raw text data into a set of numerical features that can be used for machine learning or other NLP tasks. \n",
    "\n",
    "The goal of feature extraction is to capture relevant information from the text data in a way that can be easily processed by a machine learning algorithm. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183efc67",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>What are some common ways of extraction?</b>\n",
    "\n",
    "\n",
    "__`1. Bag-of-Words (BoW):`__ BoW is a simple technique that involves representing a document as a vector of word counts. It involves counting the frequency of each word in a document and then representing the document as a vector of word counts. BoW is simple to implement and can be used for a variety of NLP tasks, such as text classification and clustering.\n",
    "\n",
    "__`2. Term Frequency-Inverse Document Frequency (TF-IDF):`__ TF-IDF is a technique that represents text as a weight that measures the importance of a word in a document. It takes into account not only the frequency of a word in a document but also its frequency across the corpus. Words that are common across the corpus but rare in a document are given higher weights, while words that are rare across the corpus but common in a document are given lower weights.\n",
    "\n",
    "__`3. Word Embeddings:`__ Word embeddings are dense vector representations of words that capture the meaning and context of words in a corpus. They are generated by training a neural network on a large corpus of text, such as Wikipedia or news articles. Word embeddings can be used for a variety of NLP tasks, such as sentiment analysis, named entity recognition, and machine translation.\n",
    "\n",
    "__`4. Named Entity Recognition (NER):`__ NER is a technique that involves identifying and extracting named entities such as person names, organization names, and locations from text. NER can be used to generate features that represent the presence or absence of named entities in a document.\n",
    "\n",
    "__`5. Part-of-Speech (POS) tagging:`__ POS tagging involves labeling the part of speech (e.g. noun, verb, adjective) for each word in a sentence. POS tags can be used to generate features that capture syntactic information about a sentence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76583ba4",
   "metadata": {},
   "source": [
    "## NLP Terminologies Primer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b78552",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    " \n",
    "Here is a list of common terminologies used in Natural Language Processing (NLP):\n",
    "\n",
    "`1. Corpus:` A collection of text documents(datapoints) or spoken words used for analysis and study in NLP. It can be considered as a pragraph having many sentences in it.\n",
    "    \n",
    "\n",
    "`2. Documents:` It is same as the sentences in a paragraph. A group of documents constitutes a pragraph/corpus.\n",
    "    \n",
    "   \n",
    "\n",
    " `3. Vocabulary:` It represents the no. of unique words present in the corpus. \n",
    "    \n",
    "\n",
    "`4. Tokenization:` The process of splitting text into individual units, usually words or subwords, for further analysis.\n",
    "    \n",
    "\n",
    "`5. Stemming:` The process of reducing a word to its base or root form.\n",
    "    \n",
    "\n",
    "`6. Lemmatization:` A more advanced version of stemming, which reduces a word to its base or dictionary form.\n",
    "    \n",
    "\n",
    "`7. Stop words:` Common words that are usually removed from text during preprocessing, as they carry little meaning.\n",
    "    \n",
    "\n",
    "`8. N-grams:` A sequence of N words that are adjacent to each other in a text.\n",
    "    \n",
    "\n",
    "`9. Part-of-speech (POS) tagging:` The process of labeling each word in a text with its corresponding part of speech.\n",
    "    \n",
    "\n",
    "`10. Named Entity Recognition (NER):` The process of identifying and labeling named entities such as people, organizations, and     locations in a text.\n",
    "    \n",
    "\n",
    "`11. Dependency parsing:` The process of analyzing the grammatical structure of a sentence by identifying the relationships between words.\n",
    "\n",
    "`12. Sentiment analysis:` The process of determining the emotional tone or attitude of a text, typically positive, negative, or neutral.\n",
    "   \n",
    "`13. Topic modeling:` The process of identifying the underlying topics or themes in a collection of text documents.\n",
    "    \n",
    "\n",
    "`14. Bag of Words (BoW):` A simple method for representing text as a collection of word frequencies.\n",
    "    \n",
    "\n",
    "`15. Term frequency-inverse document frequency (TF-IDF):` A statistical measure used to evaluate the importance of a word in a document, based on how frequently it appears in the document and how rare it is in the corpus.\n",
    "\n",
    "`16. Word embedding:` A technique for representing words as dense vectors of real numbers, which capture their semantic meaning and relationships.\n",
    "\n",
    "`17. Neural machine translation:` A method of machine translation that uses neural networks to learn the mapping between languages.\n",
    "\n",
    "`18. Language models:` Models that estimate the probability of a sequence of words in a language, typically used for tasks such as speech recognition and text generation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d4b48",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f8207",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0da81c",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__`Pros:`__\n",
    "\n",
    "`1. Simplicity:` BoW is a simple and intuitive method for representing text as numerical vectors, making it easy to understand and implement.\n",
    "\n",
    "`2. Flexibility:` BoW can be used with a variety of machine learning algorithms, making it a versatile tool for text analysis.\n",
    "\n",
    "`3. Efficiency:` BoW can be computationally efficient, especially when used with sparse representations.\n",
    "\n",
    "`4. Robustness:` BoW is robust to changes in word order and grammar, making it suitable for a wide range of text data.\n",
    "    \n",
    "__`Cons:`__\n",
    "\n",
    "`1.  Loss of information:` BoW treats each word in the text as independent and ignores the order of the words, resulting in the loss of important contextual information.\n",
    "\n",
    "`2. Vocabulary size:` BoW can result in a high-dimensional vector representation, especially when using a large vocabulary, which can make it computationally expensive and difficult to interpret.\n",
    "\n",
    "`3. Out-of-vocabulary words:` BoW can struggle with words that are not present in the training vocabulary, which can result in inaccurate representations.\n",
    "\n",
    "`4. Semantic ambiguity:` BoW does not capture the nuances of word meanings, which can result in multiple words with similar meanings being represented as separate features.\n",
    "</d>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f02276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c611b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"This is the first sentence. This is the second sentence. And this is the third sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb39f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is the first sentence. This is the second sentence. And this is the third sentence."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(para)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ec9ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'> --> This is the first sentence.\n",
      "<class 'str'> --> This is the first sentence.\n",
      "<class 'spacy.tokens.span.Span'> --> This is the second sentence.\n",
      "<class 'str'> --> This is the second sentence.\n",
      "<class 'spacy.tokens.span.Span'> --> And this is the third sentence.\n",
      "<class 'str'> --> And this is the third sentence.\n"
     ]
    }
   ],
   "source": [
    "for i in doc.sents:\n",
    "    print(f\"{type(i)} --> {i}\")\n",
    "    print(f\"{type(i.text)} --> {i.text}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991d0d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'This is the second sentence.',\n",
       " 'And this is the third sentence.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [i.text for i in doc.sents]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6966cb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1,1), max_features=None)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9ba5c",
   "metadata": {},
   "source": [
    "__`Note:`__ An ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70837b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'first', 'is', 'second', 'sentence', 'the', 'third', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893b9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af6ffac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 7,\n",
       " 'is': 2,\n",
       " 'the': 5,\n",
       " 'first': 1,\n",
       " 'sentence': 4,\n",
       " 'second': 3,\n",
       " 'and': 0,\n",
       " 'third': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ee0846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'This is the second sentence.',\n",
       " 'And this is the third sentence.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d0f2c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee5fab",
   "metadata": {},
   "source": [
    "### Visualize the Bag Of Words \n",
    " \n",
    " The vocabulary dictionary being generated contains the words as keys and the values are the position in the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe196482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  first  is  second  sentence  the  third  this\n",
       "0    0      1   1       0         1    1      0     1\n",
       "1    0      0   1       1         1    1      0     1\n",
       "2    1      0   1       0         1    1      1     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968903f1",
   "metadata": {},
   "source": [
    "### Tweak with ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf8c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'This is the second sentence.',\n",
       " 'And this is the third sentence.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe36a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and this</th>\n",
       "      <th>first sentence</th>\n",
       "      <th>is the</th>\n",
       "      <th>second sentence</th>\n",
       "      <th>the first</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third sentence</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and this  first sentence  is the  second sentence  the first  the second   \n",
       "0         0               1       1                0          1           0  \\\n",
       "1         0               0       1                1          0           1   \n",
       "2         1               0       1                0          0           0   \n",
       "\n",
       "   the third  third sentence  this is  \n",
       "0          0               0        1  \n",
       "1          0               0        1  \n",
       "2          1               1        1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(2,2), max_features=None)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f604d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and this</th>\n",
       "      <th>first</th>\n",
       "      <th>first sentence</th>\n",
       "      <th>is</th>\n",
       "      <th>is the</th>\n",
       "      <th>second</th>\n",
       "      <th>second sentence</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>the first</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third</th>\n",
       "      <th>third sentence</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  and this  first  first sentence  is  is the  second  second sentence   \n",
       "0    0         0      1               1   1       1       0                0  \\\n",
       "1    0         0      0               0   1       1       1                1   \n",
       "2    1         1      0               0   1       1       0                0   \n",
       "\n",
       "   sentence  the  the first  the second  the third  third  third sentence   \n",
       "0         1    1          1           0          0      0               0  \\\n",
       "1         1    1          0           1          0      0               0   \n",
       "2         1    1          0           0          1      1               1   \n",
       "\n",
       "   this  this is  \n",
       "0     1        1  \n",
       "1     1        1  \n",
       "2     1        1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1,2), max_features=None)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14387e46",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e91dc",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "TF-IDF is a more sophisticated and effective technique for text analysis than Bag-of-Words, as it gives more importance to the words that are important in the document and less importance to the common words.\n",
    "\n",
    "It helps to overcome some of the limitations of Bag-of-Words model by giving more importance to the words that are important and less importance to the common words. \n",
    "\n",
    "\n",
    "__`Term Frequency (TF)`__\n",
    "    \n",
    "TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
    "\n",
    "__`Inverse Document Frequency (IDF)`__\n",
    "    \n",
    "IDF(t, D) = log_e(Total number of documents in the corpus D / Number of documents with term t in it)\n",
    "\n",
    "__`TF-IDF`__\n",
    "    \n",
    "TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)\n",
    "\n",
    "where:\n",
    "\n",
    "t: term(word)\n",
    "\n",
    "d: document(sentence)\n",
    "\n",
    "D: corpus (collection of documents/sentence)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829115a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__`Pros:`__\n",
    "\n",
    "`1. Gives importance to important words:` TF-IDF down-weights the common words and up-weights the important words, making it more effective in capturing the essence of the document.\n",
    "\n",
    "`2. Handles long documents better:` TF-IDF normalizes the term frequency by the inverse document frequency, giving more importance to the words that are important in the document, and less importance to the words that are common, which makes it better suited for handling long documents.\n",
    "\n",
    "`3. Improves search relevance:` Because TF-IDF gives more weight to the important words and less weight to the common words, it can help improve the relevance of search results.\n",
    "\n",
    "`4. Widely used:` TF-IDF is a widely used technique in text mining and information retrieval.\n",
    "    \n",
    "    \n",
    "\n",
    "__`Cons:`__\n",
    "\n",
    "`1. Assumes independence between terms:` TF-IDF assumes that the terms in a document are independent of each other, which is not always true. This can lead to inaccurate results in some cases.\n",
    "\n",
    "`2. Ignores word order and context:` TF-IDF does not take into account the order of words or the context in which they appear, which can limit its usefulness in some cases.\n",
    "\n",
    "`3. Requires a large corpus:` To compute the inverse document frequency, TF-IDF requires a large corpus of documents, which can be a limitation in some cases.\n",
    "\n",
    "`4. Can be sensitive to document length:` The inverse document frequency is sensitive to document length, which can lead to inaccuracies in some cases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5eccf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'This is the second sentence.',\n",
       " 'And this is the third sentence.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad42444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e99d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4269d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tfidf_vec.fit_transform(corpus)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a12f5a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.64612892, 0.38161415, 0.        , 0.38161415,\n",
       "        0.38161415, 0.        , 0.38161415],\n",
       "       [0.        , 0.        , 0.38161415, 0.64612892, 0.38161415,\n",
       "        0.38161415, 0.        , 0.38161415],\n",
       "       [0.54270061, 0.        , 0.32052772, 0.        , 0.32052772,\n",
       "        0.32052772, 0.54270061, 0.32052772]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aec3ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'first', 'is', 'second', 'sentence', 'the', 'third', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7da7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(x.toarray(), columns=tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f24156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646129</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.646129</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.381614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.542701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320528</td>\n",
       "      <td>0.320528</td>\n",
       "      <td>0.542701</td>\n",
       "      <td>0.320528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and     first        is    second  sentence       the     third   \n",
       "0  0.000000  0.646129  0.381614  0.000000  0.381614  0.381614  0.000000  \\\n",
       "1  0.000000  0.000000  0.381614  0.646129  0.381614  0.381614  0.000000   \n",
       "2  0.542701  0.000000  0.320528  0.000000  0.320528  0.320528  0.542701   \n",
       "\n",
       "       this  \n",
       "0  0.381614  \n",
       "1  0.381614  \n",
       "2  0.320528  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba680104",
   "metadata": {},
   "source": [
    "__`NOTE:`__ It's quite clear how the weights are distributed and normalized in propper manner giving a semantic/importance to the words with respect to sentence/document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6ddba",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b9b4e",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging  (POS) \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__`Part-of-speech (POS)`__ tagging is a process in natural language processing (NLP) that involves assigning a syntactic category to each word in a sentence based on its definition, context, and relationships with other words in the sentence.\n",
    "    \n",
    "<br>\n",
    "Processing raw text intelligently is difficult: most words are rare, and it's common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. \n",
    "    \n",
    "While it's possible to solve some problems starting from only the raw characters, it's usually better to use linguistic knowledge to add useful information. That's exactly what spaCy is designed to do: you put in raw text, and get back a **Doc** object, that comes with a variety of annotations.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f991490",
   "metadata": {},
   "source": [
    "### Coarse-Grained POS Tags\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Every token is assigned a POS Tag from the following list:\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<table><tr><th>POS</th><th>DESCRIPTION</th><th>EXAMPLES</th></tr>\n",
    "    \n",
    "<tr><td>ADJ</td><td>adjective</td><td>*big, old, green, incomprehensible, first*</td></tr>\n",
    "<tr><td>ADP</td><td>adposition</td><td>*in, to, during*</td></tr>\n",
    "<tr><td>ADV</td><td>adverb</td><td>*very, tomorrow, down, where, there*</td></tr>\n",
    "<tr><td>AUX</td><td>auxiliary</td><td>*is, has (done), will (do), should (do)*</td></tr>\n",
    "<tr><td>CONJ</td><td>conjunction</td><td>*and, or, but*</td></tr>\n",
    "<tr><td>CCONJ</td><td>coordinating conjunction</td><td>*and, or, but*</td></tr>\n",
    "<tr><td>DET</td><td>determiner</td><td>*a, an, the*</td></tr>\n",
    "<tr><td>INTJ</td><td>interjection</td><td>*psst, ouch, bravo, hello*</td></tr>\n",
    "<tr><td>NOUN</td><td>noun</td><td>*girl, cat, tree, air, beauty*</td></tr>\n",
    "<tr><td>NUM</td><td>numeral</td><td>*1, 2017, one, seventy-seven, IV, MMXIV*</td></tr>\n",
    "<tr><td>PART</td><td>particle</td><td>*'s, not,*</td></tr>\n",
    "<tr><td>PRON</td><td>pronoun</td><td>*I, you, he, she, myself, themselves, somebody*</td></tr>\n",
    "<tr><td>PROPN</td><td>proper noun</td><td>*Mary, John, London, NATO, HBO*</td></tr>\n",
    "<tr><td>PUNCT</td><td>punctuation</td><td>*., (, ), ?*</td></tr>\n",
    "<tr><td>SCONJ</td><td>subordinating conjunction</td><td>*if, while, that*</td></tr>\n",
    "<tr><td>SYM</td><td>symbol</td><td>*$, %, §, ©, +, −, ×, ÷, =, :), 😝*</td></tr>\n",
    "<tr><td>VERB</td><td>verb</td><td>*run, runs, running, eat, ate, eating*</td></tr>\n",
    "<tr><td>X</td><td>other</td><td>*sfpksdpsxmsa*</td></tr>\n",
    "<tr><td>SPACE</td><td>space</td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369873c7",
   "metadata": {},
   "source": [
    "### Fine-Grained POS Tags\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Tokens are subsequently given a fine-grained tag as determined by morphology:\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "<tr><th>POS</th><th>Description</th><th>Fine-grained Tag</th><th>Description</th><th>Morphology</th></tr>\n",
    "<tr><td>ADJ</td><td>adjective</td><td>AFX</td><td>affix</td><td>Hyph=yes</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>JJ</td><td>adjective</td><td>Degree=pos</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>JJR</td><td>adjective, comparative</td><td>Degree=comp</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>JJS</td><td>adjective, superlative</td><td>Degree=sup</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>PDT</td><td>predeterminer</td><td>AdjType=pdt PronType=prn</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>PRP\\$</td><td>pronoun, possessive</td><td>PronType=prs Poss=yes</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>WDT</td><td>wh-determiner</td><td>PronType=int rel</td></tr>\n",
    "<tr><td>ADJ</td><td></td><td>WP\\$</td><td>wh-pronoun, possessive</td><td>Poss=yes PronType=int rel</td></tr>\n",
    "<tr><td>ADP</td><td>adposition</td><td>IN</td><td>conjunction, subordinating or preposition</td><td></td></tr>\n",
    "<tr><td>ADV</td><td>adverb</td><td>EX</td><td>existential there</td><td>AdvType=ex</td></tr>\n",
    "<tr><td>ADV</td><td></td><td>RB</td><td>adverb</td><td>Degree=pos</td></tr>\n",
    "<tr><td>ADV</td><td></td><td>RBR</td><td>adverb, comparative</td><td>Degree=comp</td></tr>\n",
    "<tr><td>ADV</td><td></td><td>RBS</td><td>adverb, superlative</td><td>Degree=sup</td></tr>\n",
    "<tr><td>ADV</td><td></td><td>WRB</td><td>wh-adverb</td><td>PronType=int rel</td></tr>\n",
    "<tr><td>CONJ</td><td>conjunction</td><td>CC</td><td>conjunction, coordinating</td><td>ConjType=coor</td></tr>\n",
    "<tr><td>DET</td><td>determiner</td><td>DT</td><td>determiner</td><td></td></tr>\n",
    "<tr><td>INTJ</td><td>interjection</td><td>UH</td><td>interjection</td><td></td></tr>\n",
    "<tr><td>NOUN</td><td>noun</td><td>NN</td><td>noun, singular or mass</td><td>Number=sing</td></tr>\n",
    "<tr><td>NOUN</td><td></td><td>NNS</td><td>noun, plural</td><td>Number=plur</td></tr>\n",
    "<tr><td>NOUN</td><td></td><td>WP</td><td>wh-pronoun, personal</td><td>PronType=int rel</td></tr>\n",
    "<tr><td>NUM</td><td>numeral</td><td>CD</td><td>cardinal number</td><td>NumType=card</td></tr>\n",
    "<tr><td>PART</td><td>particle</td><td>POS</td><td>possessive ending</td><td>Poss=yes</td></tr>\n",
    "<tr><td>PART</td><td></td><td>RP</td><td>adverb, particle</td><td></td></tr>\n",
    "<tr><td>PART</td><td></td><td>TO</td><td>infinitival to</td><td>PartType=inf VerbForm=inf</td></tr>\n",
    "<tr><td>PRON</td><td>pronoun</td><td>PRP</td><td>pronoun, personal</td><td>PronType=prs</td></tr>\n",
    "<tr><td>PROPN</td><td>proper noun</td><td>NNP</td><td>noun, proper singular</td><td>NounType=prop Number=sign</td></tr>\n",
    "<tr><td>PROPN</td><td></td><td>NNPS</td><td>noun, proper plural</td><td>NounType=prop Number=plur</td></tr>\n",
    "<tr><td>PUNCT</td><td>punctuation</td><td>-LRB-</td><td>left round bracket</td><td>PunctType=brck PunctSide=ini</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>-RRB-</td><td>right round bracket</td><td>PunctType=brck PunctSide=fin</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>,</td><td>punctuation mark, comma</td><td>PunctType=comm</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>:</td><td>punctuation mark, colon or ellipsis</td><td></td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>.</td><td>punctuation mark, sentence closer</td><td>PunctType=peri</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>''</td><td>closing quotation mark</td><td>PunctType=quot PunctSide=fin</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>\"\"</td><td>closing quotation mark</td><td>PunctType=quot PunctSide=fin</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>``</td><td>opening quotation mark</td><td>PunctType=quot PunctSide=ini</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>HYPH</td><td>punctuation mark, hyphen</td><td>PunctType=dash</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>LS</td><td>list item marker</td><td>NumType=ord</td></tr>\n",
    "<tr><td>PUNCT</td><td></td><td>NFP</td><td>superfluous punctuation</td><td></td></tr>\n",
    "<tr><td>SYM</td><td>symbol</td><td>#</td><td>symbol, number sign</td><td>SymType=numbersign</td></tr>\n",
    "<tr><td>SYM</td><td></td><td>\\$</td><td>symbol, currency</td><td>SymType=currency</td></tr>\n",
    "<tr><td>SYM</td><td></td><td>SYM</td><td>symbol</td><td></td></tr>\n",
    "<tr><td>VERB</td><td>verb</td><td>BES</td><td>auxiliary \"be\"</td><td></td></tr>\n",
    "<tr><td>VERB</td><td></td><td>HVS</td><td>forms of \"have\"</td><td></td></tr>\n",
    "<tr><td>VERB</td><td></td><td>MD</td><td>verb, modal auxiliary</td><td>VerbType=mod</td></tr>\n",
    "<tr><td>VERB</td><td></td><td>VB</td><td>verb, base form</td><td>VerbForm=inf</td></tr>\n",
    "<tr><td>VERB</td><td></td><td>VBD</td><td>verb, past tense</td><td>VerbForm=fin Tense=past</td></tr>\n",
    "<tr><td>VERB</td><td></td><td>VBG</td><td>verb, gerund or present participle</td><td>VerbForm=part Tense=pres Aspect=prog</td></tr>\n",
    "<tr><td>VERB</td><td></td><td>VBN</td><td>verb, past participle</td><td>VerbForm=part Tense=past Aspect=perf</td></tr>\n",
    "<tr><td>VERB</td><td></td><td>VBP</td><td>verb, non-3rd person singular present</td><td>VerbForm=fin Tense=pres</td></tr>\n",
    "<tr><td>VERB</td><td></td><td>VBZ</td><td>verb, 3rd person singular present</td><td>VerbForm=fin Tense=pres Number=sing Person=3</td></tr>\n",
    "<tr><td>X</td><td>other</td><td>ADD</td><td>email</td><td></td></tr>\n",
    "<tr><td>X</td><td></td><td>FW</td><td>foreign word</td><td>Foreign=yes</td></tr>\n",
    "<tr><td>X</td><td></td><td>GW</td><td>additional word in multi-word expression</td><td></td></tr>\n",
    "<tr><td>X</td><td></td><td>XX</td><td>unknown</td><td></td></tr>\n",
    "<tr><td>SPACE</td><td>space</td><td>_SP</td><td>space</td><td></td></tr>\n",
    "<tr><td></td><td></td><td>NIL</td><td>missing tag</td><td></td></tr>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261f3a1",
   "metadata": {},
   "source": [
    "* __`token.pos_ :`__ To view the coarse POS tag.\n",
    "* __`token.tag_ :`__ To view the fine-grained tag.\n",
    "* __`spacy.explain(tag) :`__ To view the description of either type of tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa6946",
   "metadata": {},
   "source": [
    "__`Note :`__ You can obtain a particular token by its index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd2da399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read       VERB     VBP    verb, non-3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I read books on NLP.')\n",
    "r = doc[1]\n",
    "\n",
    "print(f'{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f210a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read       VERB     VBD    verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I read a book on NLP.')\n",
    "r = doc[1]\n",
    "\n",
    "print(f'{r.text:{10}} {r.pos_:{8}} {r.tag_:{6}} {spacy.explain(r.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dba8b",
   "metadata": {},
   "source": [
    "#### Counting POS Tags\n",
    "__`The Doc.count_by()`__ method accepts a specific token attribute as its argument, and returns a frequency count of the given attribute as a dictionary object. Keys in the dictionary are the integer values of the given attribute ID, and values are the frequency. Counts of zero are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9abf42fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 2, 84: 3, 92: 3, 100: 1, 85: 1, 94: 1, 97: 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n",
    "# Count the frequencies of different coarse-grained POS tags:\n",
    "POS_counts = doc.count_by(spacy.attrs.POS)\n",
    "POS_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8cc0fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LANG'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get POS tag by its ID\n",
    "doc.vocab[83].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93e4aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84. ADJ  : 3\n",
      "85. ADP  : 1\n",
      "90. DET  : 2\n",
      "92. NOUN : 3\n",
      "94. PART : 1\n",
      "97. PUNCT: 1\n",
      "100. VERB : 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(POS_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{5}}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d05d2",
   "metadata": {},
   "source": [
    "#### Counting Fine-Grained Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0fca241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74. POS : 1\n",
      "1292078113972184607. IN  : 1\n",
      "10554686591937588953. JJ  : 3\n",
      "12646065887601541794. .   : 1\n",
      "15267657372422890137. DT  : 2\n",
      "15308085513773655218. NN  : 3\n",
      "17109001835818727656. VBD : 1\n"
     ]
    }
   ],
   "source": [
    "# Count the different fine-grained tags:\n",
    "TAG_counts = doc.count_by(spacy.attrs.TAG)\n",
    "\n",
    "for k,v in sorted(TAG_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f520c74f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd18234",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__`NER (Named Entity Recognition)`__ is a natural language processing (NLP) technique that involves identifying and categorizing named entities in text into predefined categories such as person names, organizations, locations, medical codes, etc. The goal of NER is to extract useful information from unstructured text data by identifying important named entities and classifying them into meaningful categories.\n",
    "\n",
    "<br>\n",
    "For example, consider the following sentence:\n",
    "\n",
    "__\"Barack Obama was the 44th President of the United States.\"__\n",
    "\n",
    "NER would involve identifying \"Barack Obama\" as a person name and \"the United States\" as a location.\n",
    "\n",
    "NER is a useful technique in a variety of NLP applications such as information extraction, text classification, and question answering. Many NLP libraries, including spaCy and NLTK, provide pre-trained models for NER that can be used out of the box, or trained on custom data to recognize specific entities relevant to a particular domain or application.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb8222",
   "metadata": {},
   "source": [
    "### Entity annotations\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "`Doc.ents` are token spans with their own set of annotations.\n",
    "    \n",
    "<br>\n",
    "    <br>\n",
    "<table>\n",
    "<tr><td>`ent.text`</td><td>The original entity text</td></tr>\n",
    "<tr><td>`ent.label`</td><td>The entity type's hash value</td></tr>\n",
    "<tr><td>`ent.label_`</td><td>The entity type's string description</td></tr>\n",
    "<tr><td>`ent.start`</td><td>The token span's *start* index position in the Doc</td></tr>\n",
    "<tr><td>`ent.end`</td><td>The token span's *stop* index position in the Doc</td></tr>\n",
    "<tr><td>`ent.start_char`</td><td>The entity text's *start* index position in the Doc</td></tr>\n",
    "<tr><td>`ent.end_char`</td><td>The entity text's *stop* index position in the Doc</td></tr>\n",
    "</table>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "473c6e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 dollars 4 6 20 31 MONEY\n",
      "Microsoft 11 12 53 62 ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db486d7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Tags are accessible through the `.label_` property of an entity.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\n",
    "<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\n",
    "<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\n",
    "<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\n",
    "<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\n",
    "<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\n",
    "<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\n",
    "<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\n",
    "<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\n",
    "<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\n",
    "<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\n",
    "<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\n",
    "<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\n",
    "<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\n",
    "<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\n",
    "<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\n",
    "<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\n",
    "<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\n",
    "<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e2e98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to display basic entity info:\n",
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        print('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d742ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington, DC - GPE - Countries, cities, states\n",
      "next May - DATE - Absolute or relative dates or periods\n",
      "the Washington Monument - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3693e",
   "metadata": {},
   "source": [
    "### Adding a Named Entity to a Span\n",
    "Normally we would have spaCy build a library of named entities by training it on several samples of text.<br>In this case, we only want to add one value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77c0bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K. - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n",
    "\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8cd865c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(U.K., $6 million)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents #Right now, spaCy does not recognize \"Tesla\" as a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "203d5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Get the hash value of the ORG entity label\n",
    "ORG = doc.vocab.strings[u'ORG']  \n",
    "\n",
    "# Create a Span for the new entity\n",
    "new_ent = Span(doc, 0, 1, label=ORG)\n",
    "\n",
    "# Add the entity to the existing Doc object\n",
    "doc.ents = list(doc.ents) + [new_ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57aa53c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tesla, U.K., $6 million)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f46d8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla - ORG - Companies, agencies, institutions, etc.\n",
      "U.K. - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8233b6be",
   "metadata": {},
   "source": [
    "### Counting Entities\n",
    "While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "479042ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.50 - MONEY - Monetary values, including unit\n",
      "five dollars - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Originally priced at $29.50, the sweater was marked down to five dollars.')\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74e4184f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ent for ent in doc.ents if ent.label_=='MONEY'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
